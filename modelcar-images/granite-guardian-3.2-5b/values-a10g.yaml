inferenceService:
  args:
    - "--gpu-memory-utilization=0.99"
    - "--max-model-len=24500"
